---
jupyter:
  jupytext:
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Метрические методы классификации


### Цель работы

изучение принципов построения информационных систем с использованием метрических методов классификации.

### Задачи 

 - изучение инструментария Python для реализации алгоритмов метрической классификации;
 - изучение методов оптимизации параметров метрической классификации;
 - освоение модификаций kNN-метода.

### Продолжительность и сроки сдачи

Продолжительность работы: - 4 часа.

Мягкий дедлайн (5 баллов): 24.10.2023

Жесткий дедлайн (2.5 баллов): 07.11.2023


### Теоретические сведения


Перед выполнением лабораторной работы необходимо ознакомиться с базовыми принципами работы со специализированными библиотеками яхыка Python, используя следующие источники: [1]

Перед выполнением лабораторной работы необходимо ознакомиться с базовыми принципами работы с репозитариями  [2, 3]


### Методика и порядок выполнения работы
Перед выполнением индивидуального задания рекомендуется выполнить все пункты учебной задачи.


### Учебная задача

В рамках данной задачи рассматривается построение классификатора с использованием метода ближайших соседей.
В качестве набора данных используются данные об ирисах Фишера.
В рамках данной лабораторной работы рекомендуется использование библиотеки pandas.
Pandas – это библиотека Python, предоставляющая широкие возможности для анализа данных.
Данные, с которыми работают специалисты Data Science, часто хранятся в табличном формате (.csv, .tsv, .xlsx, ...).
С помощью библиотеки Pandas данные удобно загружать, обрабатывать и анализировать с помощью SQL-подобных запросов.
Pandas предоставляет широкие возможности визуального анализа табличных данных в связке с библиотеками Matplotlib и Seaborn.

Основными структурами данных в Pandas являются классы Series и DataFrame.
Первый из них представляет собой одномерный индексированный массив данных некоторого фиксированного типа.
Второй – это двухмерная структура данных, представляющая собой таблицу, каждый столбец которой содержит данные одного типа.
Можно представлять её как словарь объектов типа Series.
Структура DataFrame отлично подходит для представления реальных данных: строки соответствуют признаковым описаниям отдельных объектов, а столбцы соответствуют признакам.


#### Подключение библиотек

```python
import numpy as np
import pandas as pd
import seaborn as sns

from matplotlib import pyplot as plt
%matplotlib inline
```

#### Получение данных
Необходимо скачать набор данных из одного из репозиториев (необходим только один текстовый файл с данными измерений): 

https://www.kaggle.com/datasets/uciml/iris

http://archive.ics.uci.edu/ml/datasets/Iris.

Заметим, что набор данных Iris доступен в пакете sklearn


#### Загрузка данных
Рассмотрим основные признаки, представленный в наборе.
Загрузим набор данных с использованием `pandas` и выведем признаки набора данных

```python
data_source = "./datasets/iris/iris.data"
data = pd.read_csv(data_source, delimiter=',')
data.head(10)
```

Слудует обратить внимание, что первая строка набора данных интерпретировалась как шапка таблицы (название столбцов).
Данную неточность необходимо исправить 

```python
data = pd.read_csv(data_source, delimiter=',', header=None)
data.head(10)
```

В этом случае столбцы в качестве названий получат порядковые номера.


Для удобства также можно дать символьные имена столбцам при загрузке

```python
data = pd.read_csv(data_source, 
                   delimiter=',', 
                   names=['sepal_length',
                          'sepal_width',
                          'petal_length',
                          'petal_width','answer'],
                   header=None)
data.head(10)
```

#### Визуализация данных


После загрузки данных можно визуализировать полученныйы набор данных. 
Для визуализации будем использовать библиотеку seaborn

```python
sns.pairplot(data);
```

На графике попарного распределения видно преимущество символьного обозначения столбцов – график легче интерпретировать. 
Отдельные классы не отмечаются различными цветами, но видно, что на отдельных подграфиках множества точек разделены. 
Следует обратить внимание на подграфики, расположенные по диаганали. 
Подумайте, что они отображают?


Для придания отдельным классам своих цветов необходимо указать, по какому признаку разделяются точки

```python
sns.pairplot(data, hue='answer');
```

Можно изменить маркеры каждого класса 

```python
sns.pairplot(data, hue='answer', markers=["o", "s", "D"]);
```

#### Построение модели

Перейдем к построению модели.
Модель метрической классификации должна обеспечивать следующий алгоритм работы: пользователь вводит новое признаковое описание объекта (объект отсутствует в обучающей выборке), а алгоритм классификации относит новый объект к одному из классов ирисов.

Существует несколько вариаций метода ближайших соседей.
Каждая модель предполагает наличие различных параметров для оптимизации.
Воспользуемся библиотекой scikit для построения классификатора

```python
from sklearn.neighbors import KNeighborsClassifier

X_train = data[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values
y_train = data['answer'].values

K = 3 # Количество соседей

# Создание и настройка классификатора
knn = KNeighborsClassifier(n_neighbors=K)
# построение модели классификатора (процедура обучения)
knn.fit(X_train, y_train)

# Использование классификатора
# Объявление признаков объекта
X_new = np.array([[1.2, 1.0, 2.8, 1.2]])
# Получение ответа для нового объекта
target = knn.predict(X_new)
print(target)
```

Для представленного объекта X_test попробуйте поменять значение признаков и проследите за изменением значения выходного класса.


Модель построена и выдает ответ для новых (отсутствующих в исходной выборке) объектов. 
Но, анализируя используемый код, следует отметить следующие недостатки такого подхода:
- в качестве количества ближайших соседей выбрано значение K=3, выбор данного значения не обосновывается, но в данном методе именно данный параметр должен оптимизироваться;
- отсутствует какое-либо графическое представление модели, нет визуализации процесса принятия решения.

Исправим данные недостатки.
Займемся обоснованием выбора оптимального значения количества ближайших соседей. 
Для этого будем использовать простейшую оценку качества hold-out.

Заодно, покажем как получит набор данных из библиотеки sklearn

```python
from sklearn import datasets

iris =  datasets.load_iris()
data = pd.DataFrame(iris.data, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])
data['answer'] = iris.target
```

```python
# Оценка точности классификатора с использованием методики hold-out


from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X_train, X_holdout, y_train, y_holdout = train_test_split(
    data[
    ['sepal_length',
     'sepal_width',
     'petal_length',
     'petal_width']], 
    data['answer'], 
    test_size=0.3, 
    random_state=17)

K = 3 # Количество соседей
knn = KNeighborsClassifier(n_neighbors=K)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_holdout)
accur = accuracy_score(y_holdout, knn_pred)
print('accuracy: ', accur)
```

В качестве эксперимента попробуйте поменять значение количества соседей и расмотрите изменение точности класификатора.


Еще одна оценка качества – cross validation (CV) error.

```python
# Реализация процедуры выбора оптимального параметра на основе cross validation error
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

# Значения параметра K
k_list = list(range(1,50))
# Пустой список для хранения значений точности
cv_scores = []
# В цикле проходим все значения K
for K in k_list:
    knn = KNeighborsClassifier(n_neighbors=K)
    scores = cross_val_score(
        knn, 
        data.iloc[ : , 0:4 ], 
        data['answer'], 
        cv=10, 
        scoring='accuracy')
    cv_scores.append(scores.mean())

# Вычисляем ошибку (misclassification error)
MSE = [1-x for x in cv_scores]

# Строим график
plt.plot(k_list, MSE)
plt.xlabel('Количество соседей (K)');
plt.ylabel('Ошибка классификации (MSE)')
plt.show()

# Ищем минимум
k_min = min(MSE)

# Пробуем найти прочие минимумы (если их несколько)
all_k_min = []
for i in range(len(MSE)):
    if MSE[i] <= k_min:
        all_k_min.append(k_list[i])

# печатаем все K, оптимальные для модели
print('Оптимальные значения K: ', all_k_min)
```

Визуализируем полученные результаты

```python
dX = data.iloc[:,0:4]
dy = data['answer']

plot_markers = ['r*', 'g^', 'bo']
answers = dy.unique()

# Создаем подграфики для каждой пары признаков
f, places = plt.subplots(4, 4, figsize=(16,16))

fmin = dX.min()-0.5
fmax = dX.max()+0.5
plot_step = 0.02  

# Обходим все subplot
for i in range(0,4):
    for j in range(0,4):

        # Строим решающие границы 
        if(i != j):
            xx, yy = np.meshgrid(np.arange(fmin[i], fmax[i], plot_step),
                               np.arange(fmin[j], fmax[j], plot_step))
            model = KNeighborsClassifier(n_neighbors=13)
            model.fit(dX.iloc[:, [i,j]].values, dy.values)
            p = model.predict(np.c_[xx.ravel(), yy.ravel()])
            p = p.reshape(xx.shape)
            places[i,j].contourf(xx, yy, p, cmap='Pastel1') 
      
        # Обход всех классов
        for id_answer in range(len(answers)):
            idx = np.where(dy == answers[id_answer])
            if i==j:
                places[i, j].hist(dX.iloc[idx].iloc[:,i],
                                  color=plot_markers[id_answer][0],
                                 histtype = 'step')
            else:
                places[i, j].plot(dX.iloc[idx].iloc[:,i], dX.iloc[idx].iloc[:,j], 
                                  plot_markers[id_answer], 
                                  label=answers[id_answer], markersize=6)
        
        if j==0:
            places[i, j].set_ylabel(dX.columns[j])
        
        if i==3:
            places[i, j].set_xlabel(dX.columns[i])        
```

#### Использование модели

```python
from sklearn.neighbors import KNeighborsClassifier

X_train = data[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values
y_train = data['answer'].values

K = 13 # Найденное оптимальное значение соседей

# Создание и настройка классификатора
knn = KNeighborsClassifier(n_neighbors=K)
# построение модели классификатора (процедура обучения)
knn.fit(X_train, y_train)

# Использование классификатора
# Объявление признаков объекта

sepal_length = float(input('Введите длину чашелистика: '))
sepal_width = float(input('Введите ширину чашелистика: '))
petal_length = float(input('Введите длину лепестка: '))
petal_width = float(input('Введите ширину лепестка: '))
X_new = np.array([[sepal_length, sepal_width, petal_length, petal_width]])

# Получение ответа для нового объекта
target = knn.predict(X_new)

if target == 0:
    iris_type = 'Iris-setosa'
elif target == 1:
    iris_type = 'Iris-versicolor'
elif target == 2:
    iris_type = 'Iris-virginica'

print('\nДанный цветок относится к виду: ', iris_type)
```

<!-- #region jp-MarkdownHeadingCollapsed=true -->
### Важные замечания

При выборе набора данных следует отдавать предпочтение тем наборам, которые имеют следующие характеристики: содержат не более 5 признаков на объект; все признаки – числовые; желательно отсутствие пропусков в данных.
<!-- #endregion -->

### Индивидуальное задание

1. Подберите набор данных на ресурсах [2, 3] и согласуйте свой выбор с преподавателем и другими студентами группы, так
как работа над одинаковыми наборами данных недопустима..

2. Выполните построение модели классификации на основе метода ближайших соседей. В ходе решения задачи необходимо решить следующие подзадачи:
 
 - Построение классификатора с заданием K (количества ближайших соседей) пользователем; 
 - Вычисление оценки hold-out для различнх значений K, а также для различных долей обучающей и тестирующей подвыборок;
 - Вычисление оценки cross validation для различных значений K, а также для различных значений fold (количества подмножеств при кроссвалидации).
 - Вычислите оптимальные значения K. Обоснуйте свой выбор. Продемонстрируйте использование полученного классификатора.


### Содержание отчета и его форма

Отчет по лабораторной работе должен содержать:

1. Номер и название лабораторной работы; задачи лабораторной работы.

2. Реализация каждого пункта подраздела «Индивидуальное задание» с приведением исходного кода программы, диаграмм и графиков для визуализации данных.

3. Ответы на контрольные вопросы.

4. Листинг программного кода с комментариями, показывающие порядок выполнения лабораторной работы, и результаты, полученные в ходе её выполнения.


### Контрольные вопросы

1. Поясните особенности основных методов метрической классификации: метод ближайшего соседа, метод k ближайших соседей.
2. Поясните основные принципы и этапы реализации метода kNN.
3. Поясните принцип выбора количества соседних объектов, по которым определяется принадлежность целевого объекта к результирующему классу.
4. В чем заключается метод парзеновского окна?
5. Назовите, какие параметры оптимизируют в методах kNN?


### Список литературы

1. Дж. Плас: Python для сложных задач. Наука о данных и машинное обучение. Питер.,2018, 576 с.

2. [Репозиторий наборов данных для машинного обучения (Центр машинного обучения и интеллектуальных систем)](https://archive.ics.uci.edu/datasets)

3. [Репозиторий наборов данных для машинного обучения (Kaggle)](https://www.kaggle.com/datasets/)
