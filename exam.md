# Вопросы к экзамену (1 семестр)

1. Основные понятия машинного обучения. Основные постановки задач. Примеры прикладных задач. Формальная модель машинного обучения.
2. Линейные пространства. Векторы и матрицы. Линейная независимость. Обратная матрица.
3. Производная и градиент функции. Градиентный спуск. Выпуклые функции.
4. Производная и градиент функции. Алгоритм градиентного спуска.
5. Cтохастический градиентный спуск. В чём его отличия от обычного градиентного спуска? Какие у него плюсы и минусы?.
6. Регуляризация в задачах машинного обучения. Отличия L1 и L2-регуляризации. Способы бороться с переобучением. Особенности применения.
7. Обобщающая способность алгоритма машинного обучения. Недообучение и переобучение.
8. Гиперпараметры. Отличия гиперпараметров от обычных параметров алгоритмов. Приведите примеры параметров и гиперпараметров в линейных моделях.
9. Отложенная выборка. Кросс-валидация (скользящий контроль). Использование для выбора гиперпараметров.
10. Масштабирование признаков. Виды масштабирования признаков. Необходимость масштабирования признаков.
11. Функционалы качества MSE и MAE. Функция потерь Хубера. Примеры применения.
12. Метрики accuracy, precision, recall. F-мера. Преимущества F-меры перед арифметическим средним точности и полноты.
13. ROC- и PR-кривые. Способы построения. Особенности применения.
14. AUC-ROC и AUC-PRC. Способы построения. Особенности применения.
15. Случайные величины. Дискретные и непрерывные распределения. Примеры.
16. Оценивание параметров распределений, метод максимального правдоподобия. Бутстрэппинг.
17. Метрики качества алгоритм регрессии и классификации.
18. Оценивание качества алгоритмов. Отложенная выборка, ее недостатки. Оценка полного скользящего контроля. Кросс-валидация. Leave-one-out.
19. Разложения ошибки на смещение и разброс. Как бэггинг меняет смещение и разброс одной модели.
20. Методы поиска выбросов в данных. Методы восстановления пропусков в данных. Работа с несбалансированными выборками.
21. Модели обучения по прецедентам. Объекты и признаки в машинном обучении. Функционал качества в обучении по прецедентам.
22. Вещественные (числовые), бинарные, категориальные признаки.
23. Способы борьбы с переобучением модели в общем случае (независимо от модели).
24. Метрики качества используемые в случае сильного дисбаланса классов. Примеры.
25. Сведение задачи многоклассовой классификации к серии задач бинарной классификации. Примеры.
26. Одномерные подходы к отбору признаков с помощью дисперсии, корреляции и t-score. Преимущества и недостатки.
27. Отбор признаков с помощью линейных моделей, решающих деревьев, случайных лесов. Примеры.
28. Линейные методы классификации и регрессии: функционалы качества, методы настройки, особенности применения.
29. Задача кластеризации. Алгоритм K-Means. Оценки качества кластеризации.
30. Задачи классификации и регрессии. Примеры задач классификации и регрессии.
31. Случайный лес: принцип работы и примеры применения.
32. Метод k ближайших соседей с парзеновским окном. Параметры метода. Способы определения класса для нового объекта.
33. Деревья решений. Методы построения деревьев. Их регуляризация.
34. Бэггинг и метод случайных подмножеств: принцип работы и примеры применения
35. Композиции алгоритмов. Разложение ошибки на смещение и разброс.
36. Обобщенный метрический классификатор: принцип работы и примеры применения.
37. Метод ближайшего соседа в машинном обучении: принцип работы и примеры применения.
38. Метод потенциальных функций: принцип работы и примеры применения.
39. Метод парзеновского окна: принцип работы и примеры применения.
40. Оптимальный байесовский классификатор: принцип работы и примеры применения.
41. Наивный байесовский классификатор: принцип работы и примеры применения.
42. Нормальный дискриминантный анализ.
43. Линейная регрессия: принцип работы и примеры применения.
44. Линейный классификатор: принцип работы и примеры применения.
45. Метод стохастического линейного спуска: принцип работы и примеры применения.
46. Логистическая регрессия: принцип работы и примеры применения.
47. Машина опорных векторов: принцип работы и примеры применения.
48. Решающие деревья в машинном обучении: принцип работы и примеры применения.
49. Поточечные и попарные методы ранжирования. Особенности применения. Примеры использования.
50. Градиентный бустинг для среднеквадратичной ошибки: принцип работы и примеры применения.
51. Градиентный бустинг для произвольной функции потерь: принцип работы и примеры применения.
52. Что такое решающее дерево? Как оно строит прогноз для объекта? Как обучаются решающие деревья в задачах классификации и регрессии (и что такое критерии информативности)?.
53. Деревья. Критерии останова и способы выбора значений в листьях.
54. Метод главных компонент. Способы формирования новых признаков.
55. Задача кластеризации. Отличия от задач классификации и регрессии.